{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m svm\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdecomposition\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PCA\n",
      "File \u001B[1;32m~\\Code\\pythonProject\\dmtl\\venv\\lib\\site-packages\\torch\\__init__.py:855\u001B[0m\n\u001B[0;32m    853\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m special \u001B[38;5;28;01mas\u001B[39;00m special\n\u001B[0;32m    854\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackcompat\u001B[39;00m\n\u001B[1;32m--> 855\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m onnx \u001B[38;5;28;01mas\u001B[39;00m onnx\n\u001B[0;32m    856\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m jit \u001B[38;5;28;01mas\u001B[39;00m jit\n\u001B[0;32m    857\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m linalg \u001B[38;5;28;01mas\u001B[39;00m linalg\n",
      "File \u001B[1;32m~\\Code\\pythonProject\\dmtl\\venv\\lib\\site-packages\\torch\\onnx\\__init__.py:12\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_C\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _onnx \u001B[38;5;28;01mas\u001B[39;00m _C_onnx\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_C\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_onnx\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      6\u001B[0m     _CAFFE2_ATEN_FALLBACK,\n\u001B[0;32m      7\u001B[0m     OperatorExportTypes,\n\u001B[0;32m      8\u001B[0m     TensorProtoDataType,\n\u001B[0;32m      9\u001B[0m     TrainingMode,\n\u001B[0;32m     10\u001B[0m )\n\u001B[1;32m---> 12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (  \u001B[38;5;66;03m# usort:skip. Keep the order instead of sorting lexicographically\u001B[39;00m\n\u001B[0;32m     13\u001B[0m     _deprecation,\n\u001B[0;32m     14\u001B[0m     errors,\n\u001B[0;32m     15\u001B[0m     symbolic_caffe2,\n\u001B[0;32m     16\u001B[0m     symbolic_helper,\n\u001B[0;32m     17\u001B[0m     symbolic_opset7,\n\u001B[0;32m     18\u001B[0m     symbolic_opset8,\n\u001B[0;32m     19\u001B[0m     symbolic_opset9,\n\u001B[0;32m     20\u001B[0m     symbolic_opset10,\n\u001B[0;32m     21\u001B[0m     symbolic_opset11,\n\u001B[0;32m     22\u001B[0m     symbolic_opset12,\n\u001B[0;32m     23\u001B[0m     symbolic_opset13,\n\u001B[0;32m     24\u001B[0m     symbolic_opset14,\n\u001B[0;32m     25\u001B[0m     symbolic_opset15,\n\u001B[0;32m     26\u001B[0m     symbolic_opset16,\n\u001B[0;32m     27\u001B[0m     symbolic_opset17,\n\u001B[0;32m     28\u001B[0m     utils,\n\u001B[0;32m     29\u001B[0m )\n\u001B[0;32m     31\u001B[0m \u001B[38;5;66;03m# TODO(After 1.13 release): Remove the deprecated SymbolicContext\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_exporter_states\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ExportTypes, SymbolicContext\n",
      "File \u001B[1;32m~\\Code\\pythonProject\\dmtl\\venv\\lib\\site-packages\\torch\\onnx\\symbolic_caffe2.py:4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mimportlib\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01minspect\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01monnx\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m symbolic_helper, symbolic_opset9 \u001B[38;5;28;01mas\u001B[39;00m opset9\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01monnx\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_internal\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m jit_utils, registration\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mregister_quantized_ops\u001B[39m(domain: \u001B[38;5;28mstr\u001B[39m, version: \u001B[38;5;28mint\u001B[39m):\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;66;03m# Register all quantized ops\u001B[39;00m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1007\u001B[0m, in \u001B[0;36m_find_and_load\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:986\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:680\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[1;34m(spec)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:846\u001B[0m, in \u001B[0;36mexec_module\u001B[1;34m(self, module)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:941\u001B[0m, in \u001B[0;36mget_code\u001B[1;34m(self, fullname)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:1039\u001B[0m, in \u001B[0;36mget_data\u001B[1;34m(self, path)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier, NeighborhoodComponentsAnalysis\n",
    "from sklearn.pipeline import Pipeline\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_text\n",
    "\n",
    "from src.base.training.models.architectures.lenet import LeNet\n",
    "from src.base.training.models.architectures.lenet_light import LeNetLight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LeNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 30\u001B[0m\n\u001B[0;32m     27\u001B[0m         x_out \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((x_out, model(x)), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     28\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x_out\n\u001B[1;32m---> 30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_and_prepare\u001B[39m(n_samples\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, train\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, model_fn\u001B[38;5;241m=\u001B[39m\u001B[43mLeNet\u001B[49m):\n\u001B[0;32m     31\u001B[0m     fmnist_x, fmnist_y \u001B[38;5;241m=\u001B[39m load_samples(datasets\u001B[38;5;241m.\u001B[39mFashionMNIST, \u001B[38;5;28mint\u001B[39m(n_samples\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m2\u001B[39m), train\u001B[38;5;241m=\u001B[39mtrain)\n\u001B[0;32m     32\u001B[0m     mnist_x, mnist_y \u001B[38;5;241m=\u001B[39m load_samples(datasets\u001B[38;5;241m.\u001B[39mMNIST, \u001B[38;5;28mint\u001B[39m(n_samples\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m2\u001B[39m), train\u001B[38;5;241m=\u001B[39mtrain)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'LeNet' is not defined"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"C:\\\\Users\\\\micdu\\\\Code\\\\pythonProject\\\\dmtl\\\\data\"\n",
    "\n",
    "def load_samples(dataset_fn, n_samples, train=True):\n",
    "    dataset = dataset_fn(\n",
    "        DATA_PATH,\n",
    "        train=train,\n",
    "        download=True,\n",
    "        transform=ToTensor()\n",
    "    )\n",
    "    loader = DataLoader(dataset, batch_size=n_samples)\n",
    "    return next(iter(loader))\n",
    "\n",
    "def shuffle(x, y):\n",
    "    shuffle_index = torch.randperm(x.shape[0])\n",
    "    return x[shuffle_index], y[shuffle_index]\n",
    "\n",
    "def load_model(model_fn, path):\n",
    "    model = model_fn()\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def use_models(x, model_fn, paths):\n",
    "    x_out = load_model(model_fn, paths[0])(x)\n",
    "    for path in paths[1:]:\n",
    "        model = load_model(model_fn, path)\n",
    "        x_out = torch.cat((x_out, model(x)), dim=1)\n",
    "    return x_out\n",
    "\n",
    "def load_and_prepare(n_samples=100, train=True, model_fn=LeNet):\n",
    "    fmnist_x, fmnist_y = load_samples(datasets.FashionMNIST, int(n_samples/2), train=train)\n",
    "    mnist_x, mnist_y = load_samples(datasets.MNIST, int(n_samples/2), train=train)\n",
    "    # Off setting the fmnist labels\n",
    "    fmnist_y = fmnist_y + 10\n",
    "    x, y = shuffle(\n",
    "        torch.cat((mnist_x, fmnist_x), dim=0),\n",
    "        torch.cat((mnist_y, fmnist_y), dim=0)\n",
    "    )\n",
    "    x_out = use_models(x, model_fn, [\n",
    "        \"C:\\\\Users\\\\micdu\\\\Code\\\\pythonProject\\\\dmtl\\\\notebooks\\\\models\\\\daeclust_15\\\\5aa285fe2dad84e59107a2652432eeac66db9c709fe2719ba74bd80caa7f493a\\\\final_model.state\",\n",
    "        \"C:\\\\Users\\\\micdu\\\\Code\\\\pythonProject\\\\dmtl\\\\notebooks\\\\models\\\\daeclust_15\\\\e5307874a84923007d15c8c019aa67d7756478bd3466d17a14b856a76e6ee29d\\\\final_model.state\"\n",
    "    ])\n",
    "    return x_out.detach().numpy(), y.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_test, y_test = load_and_prepare(model_fn=LeNetLight, n_samples=5000, train=False)\n",
    "\n",
    "predicion = np.argmax(x_test, axis=1)\n",
    "classification_report(predicion, y_test, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def use_pca(x_train, y_train, x_test, y_test, train_test_fn, n_components=15):\n",
    "    if isinstance(n_components, int) and n_components > 0:\n",
    "        pca = PCA(n_components=n_components)\n",
    "        x_train = pca.fit_transform(x_train, y_train)\n",
    "        x_test = pca.transform(x_test)\n",
    "    return train_test_fn(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_decision_tree(x_train, y_train, x_test, y_test):\n",
    "    decision_tree = DecisionTreeClassifier(random_state=0, max_depth=25)\n",
    "    decision_tree = decision_tree.fit(x_train, y_train)\n",
    "    tree_pred = decision_tree.predict(x_test)\n",
    "    return classification_report(tree_pred, y_test, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_svm(x, y, x_test, y_test):\n",
    "    # LinearSVC, ovo, ovr\n",
    "    svm_clf = svm.SVC()\n",
    "    svm_clf.fit(x, y)\n",
    "    svm_pred = svm_clf.predict(x_test)\n",
    "    return classification_report(svm_pred, y_test, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "def train_test_gnb(x, y, x_test, y_test):\n",
    "    gnb = GaussianNB()\n",
    "    gnb = gnb.fit(x, y)\n",
    "    gnb_pred = gnb.predict(x_test)\n",
    "    return classification_report(gnb_pred, y_test, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/neighbors.html\n",
    "def train_test_neighbors(x_train, y_train, x_test, y_test):\n",
    "    nca = NeighborhoodComponentsAnalysis(random_state=42)\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    nca_pipe = Pipeline([('nca', nca), ('knn', knn)])\n",
    "    nca_pipe.fit(x_train, y_train)\n",
    "    nca_knn_preds = nca_pipe.predict(x_test)\n",
    "    return classification_report(nca_knn_preds, y_test, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_random_forest(x_train, y_train, x_test, y_test):\n",
    "    rnd_forest = RandomForestClassifier(max_depth=10, random_state=0)\n",
    "    rnd_forest.fit(x_train, y_train)\n",
    "    forest_pred = rnd_forest.predict(x_test)\n",
    "    return classification_report(forest_pred, y_test, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_test(n_samples, train_test_fn):\n",
    "    x_train, y_train = load_and_prepare(model_fn=LeNetLight, n_samples=n_samples, train=True)\n",
    "    x_test, y_test = load_and_prepare(model_fn=LeNetLight, n_samples=5000, train=False)\n",
    "    results = []\n",
    "    result = train_test_fn(x_train, y_train, x_test, y_test)\n",
    "    result[\"pca\"] = 0\n",
    "    result[\"n_samples\"] = n_samples\n",
    "    result[\"classifier\"] = train_test_fn.__name__\n",
    "    results.append(result)\n",
    "    pca_components = [3, 6, 9, 12, 15, 18]\n",
    "    for n_components in pca_components:\n",
    "        result = use_pca(x_train, y_train, x_test, y_test, train_test_fn, n_components=n_components)\n",
    "        result[\"pca\"] = n_components\n",
    "        result[\"n_samples\"] = n_samples\n",
    "        result[\"classifier\"] = train_test_fn.__name__\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "def train_test_all(n_samples):\n",
    "    tree_results = train_test(n_samples, train_test_decision_tree)\n",
    "    svm_results = train_test(n_samples, train_test_svm)\n",
    "    gnb_results = train_test(n_samples, train_test_gnb)\n",
    "    neighbors = train_test(n_samples, train_test_neighbors)\n",
    "    rnd_results = train_test(n_samples, train_test_random_forest)\n",
    "    return tree_results, svm_results, gnb_results, neighbors, rnd_results\n",
    "\n",
    "def train_test_samples(samples):\n",
    "    exps = []\n",
    "    for n_samples in samples:\n",
    "        tree_results, svm_results, gnb_results, neighbors, rnd_results = train_test_all(n_samples)\n",
    "        exps += tree_results\n",
    "        exps += svm_results\n",
    "        exps += gnb_results\n",
    "        exps += neighbors\n",
    "        exps += rnd_results\n",
    "    return exps\n",
    "\n",
    "exps = train_test_samples([50, 100, 200, 500, 1000, 2000, 5000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "exps_flat = [{\n",
    "    \"n_components\": exp.get(\"pca\"),\n",
    "    \"n_samples\": exp.get(\"n_samples\"),\n",
    "    \"classifier\": exp.get(\"classifier\"),\n",
    "    \"accuracy\": exp.get(\"accuracy\")\n",
    "} for exp in exps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(exps_flat)\n",
    "\n",
    "maxes = df.groupby([\"classifier\", \"n_samples\"])[\"accuracy\"].max().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "maxes[\"classifier\"] = maxes[\"classifier\"].replace('train_test_decision_tree', \"Decision Tree\")\n",
    "maxes[\"classifier\"] = maxes[\"classifier\"].replace('train_test_gnb', \"Gaussian Naive Bayesian\")\n",
    "maxes[\"classifier\"] = maxes[\"classifier\"].replace('train_test_neighbors', \"Nearest Neighbours\")\n",
    "maxes[\"classifier\"] = maxes[\"classifier\"].replace('train_test_random_forest', \"Random Forest\")\n",
    "maxes[\"classifier\"] = maxes[\"classifier\"].replace('train_test_svm', \"SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "ax = sns.lineplot(data=maxes, x=\"n_samples\", y=\"accuracy\", hue=\"classifier\")\n",
    "ax.set_title(\"Précision d'un classificateur en fonction du nombre de données\")\n",
    "ax.set_ylabel(\"Précision\")\n",
    "ax.set_xlabel(\"Nombre d'échantillons\")\n",
    "plt.savefig(\"multi_task_acc_balanced.jpg\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}